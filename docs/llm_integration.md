# LLM Integration Guide

This document outlines how the scheduling service interacts with the local Ollama LLM
instance to rank appointment slots.

## Prompt Template

The backend sends the following prompt to the model. Variables wrapped in curly
braces are interpolated at runtime.

```
You are Interpaws, an AI assistant helping a veterinary clinic schedule
appointments. A pet owner has submitted a request with the following details:
- Clinic: {clinic_name}
- Reason for visit: {reason_for_visit}
- Urgency: {urgency}
- Preferred window: {preferred_start} to {preferred_end}
- Appointment duration: {duration_minutes} minutes

You are given a list of feasible appointment slots generated by a constraint
solver. Each slot has a unique `slot_id`.

{slot_listing}

Please choose the best {max_suggestions} slots for the client. Consider:
- Matching urgency with the earliest practical time.
- Spacing appointments away from blocked hours.
- Diversity of times to give the client options.

Respond with **only** JSON using this schema:
{
  "recommendations": [
    {
      "slot_id": <integer>,
      "score": <float between 0 and 1>,
      "rationale": "<concise, human friendly explanation>"
    }
  ]
}

Return between 3 and {max_suggestions} recommendations, ordered from best to
least preferred. Do not include any extra commentary.
```

## Ollama Endpoint

The model is hosted on a local Ollama server. Requests are sent to
`POST {base_url}/api/generate` with the following payload:

```
{
  "model": "{model_name}",
  "prompt": "<rendered prompt>",
  "stream": false
}
```

The field `response` in the JSON body contains the model output. If the call
fails or the output cannot be parsed as JSON, the backend falls back to a
heuristic ranking that orders the earliest slots first.
